{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z64pTKVdGV1L"
      },
      "source": [
        "In this homework, we will implement XOR by training the two-layer Multilayer Perceptron as shown in the lecture note.\n",
        "\n",
        "Use seed=0 whenever possible.\n",
        "\n",
        "1. First consider four training examples X=[[0,0],[0,1],[1,0],[1,1]]. What should their ground truth outputs y be ?\n",
        "\n",
        "2. Now import \"MLPClassifier\" from \"sklearn.neural_network\".\n",
        "\n",
        "To instantiate the class, the key hyperparameters that have to specified are \"hidden_layer_sizes\", \"activation\", and \"learning_rate_init\".  (Read https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "Links to an external site.)\n",
        "\n",
        "The model shown in class uses \"hidden_layer_sizes=[2]\" and the step function as activation. However we need a differentiable activation function, so use \"activation='logistic'\". Also use \"random_state=0, max_iter=500, alpha=0.01\".\n",
        "\n",
        "3. Use \"learning_rate_init\" from [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]. Instantiate the model and fit the model to the training data. For each of the learning rate, report the training accuracy.\n",
        "\n",
        "4. With the best rate, retrain the model. What are the learned weights? They should be a total of 9 numbers, as you can see from the picture in the lecture note. (Hint: the weights are stored in \"coefs_\", and \"intercepts_\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quU7RxstHApi"
      },
      "source": [
        "*From the four training examples, if X = [0,0] or [1,1] it should output 0, and if X = [1,0] or [0,1] it should output 1 -> essentially a logical XOR*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5WypR16QGWVD"
      },
      "outputs": [],
      "source": [
        "# Import MLPClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3230BkvH_Si",
        "outputId": "ea738cce-abdb-413c-fb89-c0445f767f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    R    Accuracy\n",
            "-----  ----------\n",
            "0.001        0.5\n",
            "0.005        0.5\n",
            "0.01         0.75\n",
            "0.05         0.5\n",
            "0.1          0.5\n",
            "0.5          1\n",
            "1            0.75\n",
            "5            0.5\n",
            "Optimal initial learning rates: [0.5]\n"
          ]
        }
      ],
      "source": [
        "# Arrays\n",
        "rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]\n",
        "X = [[0,0],[0,1],[1,0],[1,1]]\n",
        "y = [0,1,1,0]\n",
        "errors = []\n",
        "\n",
        "# Iterate through parameter options\n",
        "for r in rates:\n",
        "  # Create classifier\n",
        "  mlp = MLPClassifier(hidden_layer_sizes = [2], activation = 'logistic', \n",
        "                      random_state = 0, max_iter = 500, alpha = 0.01, learning_rate_init = r)\n",
        "  # Fit to data\n",
        "  mlp.fit(X,y)\n",
        "  # Get accuracy, add to list\n",
        "  mlp_train_accuracy = mlp.score(X, y)\n",
        "  errors.append([r,mlp_train_accuracy])\n",
        "  \n",
        "# Print accuracies\n",
        "print(tabulate(errors, headers = [\"R\",\"Accuracy\"]))\n",
        "\n",
        "# Get best accuracy\n",
        "min_error = [e[0] for e in errors if e[1] == max([e[1] for e in errors])]\n",
        "print(\"Optimal initial learning rates:\", min_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V13cpvdrI-u4",
        "outputId": "96d6f23c-2fc6-4296-8086-8318500d84e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intercepts:\n",
            " [array([-8.1611431 ,  1.82010429]), array([3.66602896])] \n",
            "\n",
            "Coefficients:\n",
            " [array([[ 4.82877822, -3.91633033],\n",
            "       [ 4.80462945, -3.92319884]]), array([[-7.71435547],\n",
            "       [-6.76571913]])] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create tuned classifier\n",
        "mlp_opt = MLPClassifier(hidden_layer_sizes = [2], activation = 'logistic', \n",
        "                      random_state = 0, max_iter = 500, alpha = 0.01, learning_rate_init = 0.5)\n",
        "# Fit to data\n",
        "mlp_opt.fit(X,y)\n",
        "\n",
        "# Get internal coefficients\n",
        "print(\"Intercepts:\\n\",mlp_opt.intercepts_,\"\\n\")\n",
        "print(\"Coefficients:\\n\",mlp_opt.coefs_,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bxJsxqfWK2EG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:06:49) \n[Clang 12.0.1 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "4be3729a2cd8a1b0e6b1166f63eb2d13249f10a5233f501d7bc9ba2fd02b9e52"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
